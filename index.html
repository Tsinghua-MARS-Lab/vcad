<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>VCAD: Vision-Centric Autonomous Driving</title>

    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;
</script>

<body>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->

    <div class="logo" align="center">
        <!-- <a href="" target="_blank"> -->
            <img style=" width: 400pt;" src="images/vcad_logo.jpg">
        <!-- </a> -->
    </div>

    <div class="header">
        <div style="" class="title" id="lang">
            <b>VCAD</b>: Vision-Centric Autonomous Driving
        </div>
    </div>
    <!-- === Title Ends === -->

</div>
<!-- === Home Section Ends === -->


<div class="section">
    <div class="title" id="lang">What is VCAD?</div>
    <!-- <div class="logo" style="" align="center">
        <img style="width: 700pt;" src="images/DETR3D.png">
    </div> -->
    <p>
        <b>VCAD (Vision-Centric Autonomous Driving)</b> is an open-source research effort that pushes the frontiers of camera-centered autonomous driving technology. Existing autonomous driving technology is heavily relient on LiDAR sensing, however recent advances in AI show the great promise of vision-centered autonomous driving technology. Here are several important reasons to work on VCAD:
    </p>
    <ul>
        <li><b> Rich Visual Attributes.</b> Visual information contains rich attributes of the driving scene, which is essential for holistic scene understanding and decision making. Imagine a road construction scenario where you need to understand the road blocked by cones, and the behaviors of construction workers. However, such information is under-explored in previous research.
        </li>
        <li><b> Vision is Scalable.</b> Compared to LiDARs, RADARs, or other perception sensors, cameras are way more mature and pervasive. Vision technology is scalable in terms of data collection, compression, transmission and model training. </li>  
        <li><b> Vision-centric sensor fusion.</b> We believe sensor fusion is critical for the future of autonomous driving, so we are actively pushing for vision-centric multi-sensory fusion.</li> 
    </ul>
    
</div>


<div class="section">
    <div class="title" id="lang">Projects on VCAD</div>
    <div class="col text-center">

    <table width="100%" style="margin: 0pt 0pt; text-align: center;">
    <tr>
      <td>
      <a href="https://tsinghua-mars-lab.github.io/HDMapNet/" class="d-inline-block p-3"><img height="100"
          src="images/hdmapnet_thumbnail.gif" style="border:1px solid" data-nothumb><br>HDMapNet</a>
      </td>
      <td>
        <b>BEV Mapping</b>: HDMapNet is an online HD map learning framework, whose goal is replace pre-annotated HD semantic maps.
      </td>
    </tr>
    <tr  height="10"></tr>
    <tr>
      <td>
        <a href="https://tsinghua-mars-lab.github.io/detr3d/" class="d-inline-block p-3"><img height="100"
          src="images/detr3d_thumbnail.png" style="border:1px solid"
          data-nothumb><br>DETR3D</a>
      </td>
      <td>
        <b>BEV Detection</b>: DETR3D is a multi-camera 3D object detection framework that does NOT require dense depth prediction or post-processing.
      </td>

    </tr>
    <tr  height="10"></tr>
    <tr>
      <td>
      <a href="https://tsinghua-mars-lab.github.io/futr3d/" class="d-inline-block p-3"><img height="100"
          src="images/futr3d_thumbnail.png" style="border:1px solid"
          data-nothumb><br>FUTR3D</a>
      </td>

      <td>
        <b>BEV Fusion</b>: FUTR3D is a unified sensor fusion framework that works with ARBITRARY sensor combinations.
      </td>
    </tr>
    <tr  height="10"></tr>
    <tr>
      <td>
      <a href="https://tsinghua-mars-lab.github.io/mutr3d/" class="d-inline-block p-3"><img height="100"
          src="images/mutr3d_thumbnail.png" style="border:1px solid"
          data-nothumb><br>MUTR3D</a>
      </td>
      <td>
        <b>BEV Tracking</b>: MUTR3D is an end-to-end multi-camera 3D tracking framework that works with arbrtary camera rigs.
      </td>

    </tr>
    <tr  height="10"></tr>
    <tr>
      <td>
      <a href="https://tsinghua-mars-lab.github.io/vectormapnet/" class="d-inline-block p-3"><img height="100"
          src="images/VectorMapNet_thumbnail.png" style="border:1px solid"
          data-nothumb><br>VectorMapNet</a>
      </td>
      <td>
        <b>BEV Vectorized Mapping</b>: VectorMapNet is an end-to-end map learning framework that generates vectorized HD map from onboard sensor data.
      </td>

    </tr>
    </table>
    </div>

</div>

<div class="section">
    <div class="title" id="lang">Contributors</div>

    <div class="col text-center">

    <table width="100%" style="margin: 0pt 0pt; text-align: center;">
        <tr>
            <td>
            <a href="https://hangzhaomit.github.io/" class="d-inline-block p-3">
                <img height="100" src="images/hangzhao.jpg" style="border-radius: 50%;"><br>Hang Zhao<br>Tsinghua University</a>
            </td>
            <td>
            <a href="https://scholar.google.com.hk/citations?hl=en&user=nUyTDosAAAAJ" class="d-inline-block p-3">
                <img height="100" src="images/yilunwang.png" style="border-radius: 50%;"><br>Yilun Wang<br>Li Auto</a>
            </td>
            <td>
            <a href="https://people.csail.mit.edu/yuewang" class="d-inline-block p-3">
                <img height="100" src="images/yuewang.png" style="border-radius: 50%;"><br>Yue Wang<br>MIT</a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://people.csail.mit.edu/jsolomon/" class="d-inline-block p-3">
                    <img height="100" src="images/justin.png" style="border-radius: 50%;"><br>Justin Solomon<br>MIT</a>
            </td>
            <td>
            <a href="https://scholar.google.com.br/citations?user=UH9tP6QAAAAJ&hl=en" class="d-inline-block p-3">
                <img height="100" src="images/vitor.png" style="border-radius: 50%;"><br>Vitor Guizilini<br>Toyota Research Institute</a>
            </td>
            <td>
            <a href="https://tianyuanzhang.com/" class="d-inline-block p-3">
                <img height="100" src="images/tianyuan.jpg" style="border-radius: 50%;"><br>Tianyuan Zhang<br>CMU</a>
            </td>
        </tr>
        <tr>
            <td>
            <a href="https://liqi17thu.github.io/" class="d-inline-block p-3">
                <img height="100" src="images/qili.png" style="border-radius: 50%;"><br>Qi Li<br>Tsinghua University</a>
            </td>
            <td>
            <a href="#" class="d-inline-block p-3">
                <img height="100" src="images/xuanyao.png" style="border-radius: 50%;"><br>Xuanyao Chen<br>Fudan University</a>
            </td>
            <td>
                <a href="https://scholar.google.com/citations?user=vRmsgQUAAAAJ&hl=zh-CN" class="d-inline-block p-3">
                    <img height="100" src="images/yicheng.jpg" style="border-radius: 50%;"><br>Yicheng Liu<br></a>
            </td>

        </tr>
    </table>
</div>    

</body>
</html>
